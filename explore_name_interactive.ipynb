{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAME MVP Interactive Exploration\n",
    "\n",
    "This notebook lets you explore the NAME evaluation results interactively.\n",
    "\n",
    "You can:\n",
    "- Adjust parameters and re-run the analysis\n",
    "- Inspect individual learning curves\n",
    "- Compare different AI capability levels\n",
    "- Generate custom visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load data\n",
    "learning_curves = pd.read_csv(\"data/wrangled/name_learning_curves.csv\")\n",
    "horizons = pd.read_csv(\"data/wrangled/name_horizons.csv\")\n",
    "\n",
    "# Load runs (sample for memory efficiency)\n",
    "runs = []\n",
    "with open(\"data/external/name_runs.jsonl\", 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 10000:  # First 10k runs\n",
    "            runs.append(json.loads(line))\n",
    "runs_df = pd.DataFrame(runs)\n",
    "\n",
    "print(f\"Loaded {len(learning_curves)} learning curves\")\n",
    "print(f\"Loaded {len(horizons)} model horizons\")\n",
    "print(f\"Loaded {len(runs_df)} runs (sample)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore a Specific Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a task to explore\n",
    "task_id = learning_curves.iloc[25]['task_id']  # Change index to explore different tasks\n",
    "print(f\"Exploring: {task_id}\")\n",
    "\n",
    "# Get task data\n",
    "task_curve = learning_curves[learning_curves['task_id'] == task_id].iloc[0]\n",
    "task_runs = runs_df[\n",
    "    (runs_df['task_id'] == task_id) & \n",
    "    (runs_df['learner_type'] == 'human_novice')\n",
    "]\n",
    "\n",
    "# Calculate empirical success rate by attempt\n",
    "empirical = task_runs.groupby('attempt_number')['score_binarized'].agg(['mean', 'count'])\n",
    "\n",
    "# Generate fitted curve\n",
    "attempts = np.linspace(1, 10, 100)\n",
    "base = task_curve['first_attempt_success_rate']\n",
    "asymptote = task_curve['fitted_asymptote']\n",
    "lam = task_curve['fitted_lambda']\n",
    "gain = asymptote - base\n",
    "\n",
    "fitted = base + gain * (1 - np.exp(-lam * attempts))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(empirical.index, empirical['mean'], 'o-', markersize=8, \n",
    "        linewidth=2, label='Empirical (novices)', color='#1f77b4')\n",
    "ax.plot(attempts, fitted, '--', linewidth=2, alpha=0.7, \n",
    "        label='Fitted model', color='#1f77b4')\n",
    "\n",
    "# Add 50% and 80% lines\n",
    "ax.axhline(0.5, color='red', linestyle=':', alpha=0.5, label='50% threshold')\n",
    "ax.axhline(0.8, color='orange', linestyle=':', alpha=0.5, label='80% threshold')\n",
    "\n",
    "ax.set_xlabel('Attempt Number', fontsize=12)\n",
    "ax.set_ylabel('Success Rate', fontsize=12)\n",
    "ax.set_title(f'Learning Curve: {task_id}\\n' + \n",
    "             f'Difficulty: {base:.2f}, λ: {lam:.3f}, RMSE: {task_curve[\"rmse\"]:.4f}',\n",
    "             fontsize=13)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nTask Statistics:\")\n",
    "print(f\"  First-attempt success: {base:.1%}\")\n",
    "print(f\"  Final asymptote: {asymptote:.1%}\")\n",
    "print(f\"  Learning rate (λ): {lam:.3f}\")\n",
    "print(f\"  RMSE: {task_curve['rmse']:.4f}\")\n",
    "print(f\"  N for 50% success: {task_curve['n_for_50pct']:.2f} attempts\")\n",
    "print(f\"  N for 80% success: {task_curve['n_for_80pct']:.2f} attempts\")\n",
    "print(f\"  Novices sampled: {task_curve['n_novices']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare AI Models Across Difficulty Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AI performance by task difficulty\n",
    "ai_models = ['baseline-weak', 'baseline-medium', 'baseline-strong']\n",
    "colors = {'baseline-weak': '#d62728', 'baseline-medium': '#ff7f0e', 'baseline-strong': '#2ca02c'}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "for model in ai_models:\n",
    "    model_runs = runs_df[\n",
    "        (runs_df['learner_type'] == 'ai_zero_shot') & \n",
    "        (runs_df['alias'] == model)\n",
    "    ]\n",
    "    \n",
    "    # Calculate success rate by task\n",
    "    task_performance = model_runs.groupby('task_id').agg({\n",
    "        'score_binarized': 'mean',\n",
    "        'first_attempt_success_rate': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Sort by difficulty for smooth line\n",
    "    task_performance = task_performance.sort_values('first_attempt_success_rate')\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(task_performance['first_attempt_success_rate'], \n",
    "              task_performance['score_binarized'],\n",
    "              alpha=0.4, s=30, color=colors[model], label=f'{model} (points)')\n",
    "    \n",
    "    # Add smoothed trend line\n",
    "    from scipy.ndimage import uniform_filter1d\n",
    "    smoothed = uniform_filter1d(task_performance['score_binarized'].values, size=5)\n",
    "    ax.plot(task_performance['first_attempt_success_rate'], smoothed,\n",
    "           linewidth=2.5, color=colors[model], label=f'{model} (trend)', alpha=0.8)\n",
    "\n",
    "# Add diagonal reference line\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, linewidth=1.5, label='y=x (perfect match)')\n",
    "\n",
    "# Add model horizons as vertical lines\n",
    "for _, row in horizons.iterrows():\n",
    "    model = row['model']\n",
    "    diff = row['difficulty_at_50pct']\n",
    "    if np.isfinite(diff):\n",
    "        ax.axvline(diff, color=colors[model], linestyle=':', alpha=0.5, linewidth=1.5)\n",
    "        ax.text(diff, 0.95, f\"{model}\\n50% point\", \n",
    "               rotation=90, va='top', ha='right', fontsize=8, color=colors[model])\n",
    "\n",
    "ax.set_xlabel('Human First-Attempt Success Rate (Task Difficulty)', fontsize=12)\n",
    "ax.set_ylabel('AI Zero-Shot Success Rate', fontsize=12)\n",
    "ax.set_title('AI Performance vs Task Difficulty\\n(Vertical lines show where each AI reaches 50%)', \n",
    "            fontsize=13, fontweight='bold')\n",
    "ax.set_xlim(-0.05, 1.05)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=9, loc='upper left')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment: What if We Change AI Capability?\n",
    "\n",
    "Let's simulate a new AI model with custom capability and see what its NAME horizon would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simulate.novice_learning import AIZeroShotSimulator\n",
    "\n",
    "# Create a custom AI model\n",
    "custom_capability = 0.6  # Try values between 0.0 and 1.0\n",
    "print(f\"Simulating AI with capability: {custom_capability}\")\n",
    "\n",
    "ai_simulator = AIZeroShotSimulator(capability=custom_capability, random_seed=42)\n",
    "\n",
    "# Simulate on all tasks\n",
    "custom_results = []\n",
    "for _, task in learning_curves.iterrows():\n",
    "    difficulty = task['first_attempt_success_rate']\n",
    "    \n",
    "    # Simulate 20 attempts to get stable estimate\n",
    "    successes = [ai_simulator.simulate_attempt(difficulty) for _ in range(20)]\n",
    "    success_rate = np.mean(successes)\n",
    "    \n",
    "    custom_results.append({\n",
    "        'task_id': task['task_id'],\n",
    "        'difficulty': difficulty,\n",
    "        'ai_success': success_rate,\n",
    "        'n_for_50pct': task['n_for_50pct']\n",
    "    })\n",
    "\n",
    "custom_df = pd.DataFrame(custom_results)\n",
    "\n",
    "# Find where AI reaches 50% success\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "sorted_df = custom_df.sort_values('ai_success')\n",
    "if len(sorted_df) >= 2:\n",
    "    interp = interp1d(sorted_df['ai_success'], sorted_df['difficulty'], \n",
    "                     bounds_error=False, fill_value='extrapolate')\n",
    "    difficulty_at_50 = float(interp(0.5))\n",
    "    \n",
    "    # Find N-attempts at that difficulty\n",
    "    interp_n = interp1d(sorted_df['difficulty'], sorted_df['n_for_50pct'],\n",
    "                       bounds_error=False, fill_value='extrapolate')\n",
    "    n_horizon = float(interp_n(difficulty_at_50))\n",
    "    \n",
    "    print(f\"\\nResults for capability {custom_capability}:\")\n",
    "    print(f\"  Overall success rate: {custom_df['ai_success'].mean():.1%}\")\n",
    "    print(f\"  Difficulty @ 50% AI success: {difficulty_at_50:.3f}\")\n",
    "    print(f\"  N-attempt horizon: {n_horizon:.2f} attempts\")\n",
    "    print(f\"\\nInterpretation: This AI performs like a novice after {n_horizon:.1f} practice attempts\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.scatter(custom_df['difficulty'], custom_df['ai_success'], \n",
    "          alpha=0.6, s=50, color='purple', label=f'Custom AI (capability={custom_capability})')\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, linewidth=1.5, label='y=x')\n",
    "\n",
    "if 'difficulty_at_50' in locals():\n",
    "    ax.axvline(difficulty_at_50, color='purple', linestyle=':', alpha=0.5, linewidth=2)\n",
    "    ax.axhline(0.5, color='red', linestyle=':', alpha=0.5, linewidth=1)\n",
    "    ax.plot([difficulty_at_50], [0.5], 'r*', markersize=15, label='50% intersection')\n",
    "\n",
    "ax.set_xlabel('Task Difficulty (Human First-Attempt)', fontsize=12)\n",
    "ax.set_ylabel('AI Success Rate', fontsize=12)\n",
    "ax.set_title(f'Custom AI Performance (Capability={custom_capability})', fontsize=13)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Learning Rates by Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract domain from task_id\n",
    "learning_curves['domain'] = learning_curves['task_id'].str.split('/').str[0]\n",
    "\n",
    "# Compare learning rates across domains\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Difficulty by domain\n",
    "ax = axes[0]\n",
    "learning_curves.boxplot(column='first_attempt_success_rate', by='domain', ax=ax)\n",
    "ax.set_xlabel('Domain', fontsize=11)\n",
    "ax.set_ylabel('First-Attempt Success Rate', fontsize=11)\n",
    "ax.set_title('Task Difficulty by Domain', fontsize=12)\n",
    "plt.sca(ax)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Learning rate by domain\n",
    "ax = axes[1]\n",
    "learning_curves.boxplot(column='fitted_lambda', by='domain', ax=ax)\n",
    "ax.set_xlabel('Domain', fontsize=11)\n",
    "ax.set_ylabel('Learning Rate (λ)', fontsize=11)\n",
    "ax.set_title('Learning Speed by Domain', fontsize=12)\n",
    "plt.sca(ax)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary stats\n",
    "print(\"\\nDomain Statistics:\")\n",
    "domain_stats = learning_curves.groupby('domain').agg({\n",
    "    'first_attempt_success_rate': ['mean', 'std'],\n",
    "    'fitted_lambda': ['mean', 'std'],\n",
    "    'fitted_asymptote': 'mean'\n",
    "}).round(3)\n",
    "print(domain_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Re-run Analysis with Different Parameters\n",
    "\n",
    "Want to test the system with different settings? You can regenerate data with custom parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate tasks with different difficulty range\n",
    "from src.generate.synthetic_tasks import SyntheticTaskGenerator\n",
    "\n",
    "# Try different parameters\n",
    "generator = SyntheticTaskGenerator(\n",
    "    n_tasks=50,  # Fewer tasks for quick testing\n",
    "    difficulty_range=(0.1, 0.7),  # Different range\n",
    "    default_lambda=0.5,  # Faster learning\n",
    "    random_seed=123  # Different seed\n",
    ")\n",
    "\n",
    "new_tasks = generator.generate_task_suite()\n",
    "stats = generator.get_summary_statistics(new_tasks)\n",
    "\n",
    "print(\"New task suite with custom parameters:\")\n",
    "print(f\"  Tasks: {stats['total_tasks']}\")\n",
    "print(f\"  Difficulty: {stats['difficulty']['mean']:.3f} ± {stats['difficulty']['std']:.3f}\")\n",
    "print(f\"  Learning rate: {stats['learning_rate']['mean']:.3f} ± {stats['learning_rate']['std']:.3f}\")\n",
    "\n",
    "# Note: To fully regenerate the pipeline, you'd run:\n",
    "# !poetry run dvc repro plot_name_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "To explore further:\n",
    "1. Modify the cells above to look at different tasks or models\n",
    "2. Change parameters and re-run the pipeline: `!poetry run dvc repro plot_name_results`\n",
    "3. Add your own AI models by modifying `src/simulate/novice_learning.py`\n",
    "4. Add real tasks by creating a YAML file matching the schema in `data/external/name_tasks.yaml`\n",
    "\n",
    "See `docs/NAME_README.md` for complete documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
